{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pulse Rate Algorithm\n",
    "\n",
    "### Contents\n",
    "Fill out this notebook as part of your final project submission.\n",
    "\n",
    "**You will have to complete both the Code and Project Write-up sections.**\n",
    "- The [Code](#Code) is where you will write a **pulse rate algorithm** and already includes the starter code.\n",
    "   - Imports - These are the imports needed for Part 1 of the final project. \n",
    "     - [glob](https://docs.python.org/3/library/glob.html)\n",
    "     - [numpy](https://numpy.org/)\n",
    "     - [scipy](https://www.scipy.org/)\n",
    "- The [Project Write-up](#Project-Write-up) to describe why you wrote the algorithm for the specific case.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "You will be using the **Troika**[1] dataset to build your algorithm. Find the dataset under `datasets/troika/training_data`. The `README` in that folder will tell you how to interpret the data. The starter code contains a function to help load these files.\n",
    "\n",
    "1. Zhilin Zhang, Zhouyue Pi, Benyuan Liu, ‘‘TROIKA: A General Framework for Heart Rate Monitoring Using Wrist-Type Photoplethysmographic Signals During Intensive Physical Exercise,’’IEEE Trans. on Biomedical Engineering, vol. 62, no. 2, pp. 522-531, February 2015. Link\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def LoadTroikaDataset():\n",
    "    \"\"\"\n",
    "    Retrieve the .mat filenames for the troika dataset.\n",
    "\n",
    "    Review the README in ./datasets/troika/ to understand the organization of\n",
    "    the .mat files.\n",
    "\n",
    "    Returns:\n",
    "        data_fls: List of names of the .mat files that contain signal data\n",
    "        ref_fls: List of names of the .mat files that contain reference data\n",
    "        <data_fls> and <ref_fls> are ordered correspondingly, so that\n",
    "        ref_fls[5] is the reference data for data_fls[5], etc...\n",
    "    \"\"\"\n",
    "    data_dir = \"./datasets/troika/training_data\"\n",
    "    data_fls = sorted(glob.glob(data_dir + \"/DATA_*.mat\"))\n",
    "    ref_fls = sorted(glob.glob(data_dir + \"/REF_*.mat\"))\n",
    "    return data_fls, ref_fls\n",
    "\n",
    "\n",
    "def LoadTroikaDataFile(data_fl):\n",
    "    \"\"\"\n",
    "    Loads and extracts signals from a troika data file.\n",
    "\n",
    "    Usage:\n",
    "        data_fls, ref_fls = LoadTroikaDataset()\n",
    "        ppg, accx, accy, accz = LoadTroikaDataFile(data_fls[0])\n",
    "\n",
    "    Args:\n",
    "        data_fl: (str) filepath to a troika .mat file.\n",
    "\n",
    "    Returns:\n",
    "        numpy arrays for ppg, accx, accy, accz signals.\n",
    "    \"\"\"\n",
    "    data = sp.io.loadmat(data_fl)['sig']\n",
    "    return data[2:]\n",
    "\n",
    "\n",
    "def AggregateErrorMetric(pr_errors, confidence_est):\n",
    "    \"\"\"\n",
    "    Computes an aggregate error metric based on confidence estimates.\n",
    "\n",
    "    Computes the MAE at 90% availability.\n",
    "\n",
    "    Args:\n",
    "        pr_errors: a numpy array of errors between pulse rate estimates and\n",
    "            corresponding reference heart rates.\n",
    "        confidence_est: a numpy array of confidence estimates for each pulse\n",
    "            rate error.\n",
    "\n",
    "    Returns:\n",
    "        the MAE at 90% availability\n",
    "    \"\"\"\n",
    "    # Higher confidence means a better estimate. The best 90% of the estimates\n",
    "    #    are above the 10th percentile confidence.\n",
    "    percentile90_confidence = np.percentile(confidence_est, 10)\n",
    "\n",
    "    # Find the errors of the best pulse rate estimates\n",
    "    best_estimates = pr_errors[confidence_est >= percentile90_confidence]\n",
    "\n",
    "    # Return the mean absolute error\n",
    "    return np.mean(np.abs(best_estimates))\n",
    "\n",
    "\n",
    "def BPFilter(band, *args):\n",
    "    \"\"\"Bandpass Filter of all numpy arrays in args\n",
    "\n",
    "    Inputs:\n",
    "        band: (tuple) The pass band. Frequency components outside\n",
    "            the two elements in the tuple will be removed.\n",
    "        args: (tuple) Tuple of input signals as numpy arrays\n",
    "        FS: (number) Global variable. The sampling rate of all the args.\n",
    "\n",
    "    Returns:\n",
    "        out: (tuple of np-arrays) Filtered args\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for arg in args:\n",
    "        b, a = sp.signal.butter(3, band, btype='bandpass', fs=FS)\n",
    "        out.append(sp.signal.filtfilt(b, a, arg))\n",
    "    return out\n",
    "\n",
    "\n",
    "def Fourier(signal, fs, k=2):\n",
    "    \"\"\"Calculate Fourier transform\n",
    "\n",
    "    Args:\n",
    "        signal: A numpy array as input signal\n",
    "        fs: (Integer) Sample rate in Hz\n",
    "        k: (Integer) Multiplier coefficient for length of array of FFT\n",
    "\n",
    "    Returns:\n",
    "        fft: One-dimensional np-array with Fourier transform values\n",
    "        freqs: One-dimensional np-array of frequencies\n",
    "    \"\"\"\n",
    "    length = k * len(signal)\n",
    "    fft = np.abs(np.fft.rfft(signal, length))\n",
    "    freqs = np.fft.rfftfreq(length, 1 / fs)\n",
    "    return fft, freqs\n",
    "\n",
    "\n",
    "def CalcSNR(freqs, fft_mags, f0, prec=5/60, first_harmonic=False):\n",
    "    \"\"\"Calculation of the Signal-Noise Ratio\n",
    "\n",
    "    Args:\n",
    "        freqs: (np-array) Frequencies of signal\n",
    "        fft_mags: (np-array) Magnitudes of fft in frequencies\n",
    "        f0: (float) Main frequency\n",
    "        prec: (float) Half of the window in which we compute the signal power\n",
    "            (in Hz)\n",
    "        first_harmonic: (bool) If True the SNR includes first harmonic\n",
    "\n",
    "    Returns:\n",
    "        snr: (float) SNR.\n",
    "    \"\"\"\n",
    "    # Limits of window for fundamental frequency\n",
    "    ll0 = f0 - prec if (f0 - prec) > F_MIN else F_MIN\n",
    "    hl0 = f0 + prec\n",
    "\n",
    "    # Get windows\n",
    "    f0_window = (freqs > ll0) & (freqs < hl0)\n",
    "\n",
    "    # Same treatment for first harmonic if required\n",
    "    f1_window = np.zeros_like(f0_window)\n",
    "    if first_harmonic:\n",
    "        # First harmonic\n",
    "        f1 = 2 * f0\n",
    "\n",
    "        # Limits of window for first harmonic frequency\n",
    "        ll1 = f1 - prec\n",
    "        hl1 = f1 + prec if (f1 + prec) < F_MAX else F_MAX\n",
    "\n",
    "        # Get window\n",
    "        f1_window = (freqs > ll1) & (freqs < hl1)\n",
    "\n",
    "    # Compute signal power and noise power\n",
    "    signal_power = np.sum(fft_mags[(f0_window) | (f1_window)])\n",
    "    noise_power = np.sum(fft_mags[~((f0_window) | (f1_window))])\n",
    "\n",
    "    # Compute SNR\n",
    "    snr = signal_power / noise_power\n",
    "\n",
    "    return snr\n",
    "\n",
    "\n",
    "def Window(ix, *args):\n",
    "    \"\"\"Handler function to select one window data of length W_LENGTH_N\n",
    "\n",
    "    Args:\n",
    "        ix: (integer) Index of the data referenced in the arrays\n",
    "        args: (tuple) List of numpy arrays in which we want to extract the\n",
    "            window data\n",
    "\n",
    "    Returns:\n",
    "        out: (tuple) List of numpy arrays crop to the current window\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for arg in args:\n",
    "        out.append(arg[ix:ix+W_LENGTH_N])\n",
    "    return out\n",
    "\n",
    "\n",
    "def Featurize(ppg, accx, accy, accz, fs, alpha=0, beta=0):\n",
    "    \"\"\"Featurization of the accelerometer and ppg signals.\n",
    "\n",
    "    Args:\n",
    "        ppg: (np.array) ppg signal\n",
    "        accx: (np.array) x-channel of the accelerometer.\n",
    "        accy: (np.array) y-channel of the accelerometer.\n",
    "        accz: (np.array) z-channel of the accelerometer.\n",
    "        fs: (number) the sampling rate\n",
    "        alpha: (float) Multiplier coeficient to raise the lower frequency limit\n",
    "        beta: (float) Multiplier coeficient to down the upper frequency limit\n",
    "\n",
    "    Returns:\n",
    "        f_list: List of features\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate magnitude of the accelerometer\n",
    "    accm = np.sqrt(np.sum(np.square(np.vstack((accx, accy, accz))), axis=0))\n",
    "\n",
    "    # Calculate Fourier transforms of the ppg signal and of the acc. magnitude\n",
    "    ppg_fft, ppg_freqs = Fourier(ppg, fs, 4)\n",
    "    acc_fft, acc_freqs = Fourier(accm, fs, 4)\n",
    "\n",
    "    # Remove fft values outside the band F_MIN-F_MAX\n",
    "    ppg_fft[(ppg_freqs <= F_MIN * (1 + alpha)) |\n",
    "            (ppg_freqs >= F_MAX * (1 - beta))] = 0.0\n",
    "    acc_fft[(acc_freqs <= F_MIN * (1 + alpha)) |\n",
    "            (acc_freqs >= F_MAX * (1 - beta))] = 0.0\n",
    "\n",
    "    # Get 3 frequencies with highest FFT in ppg (ordered from higher\n",
    "    # to lower FFT)\n",
    "    ppg_f_maxfft1, ppg_f_maxfft2, ppg_f_maxfft3 = \\\n",
    "        ppg_freqs[np.argsort(ppg_fft, axis=0)[-3:]][::-1]\n",
    "\n",
    "    # Get 2 frequencies with highest FFT in acc (ordered from lower\n",
    "    # to higher FFT)\n",
    "    acc_f_maxfft1, acc_f_maxfft2 = acc_freqs[np.argsort(acc_fft, axis=0)[-2:]]\n",
    "\n",
    "    # The pearson correlation of ppg and channel z of acc\n",
    "    corr_zp = sp.stats.pearsonr(accz, ppg)[0]\n",
    "\n",
    "    # Compute the energy spectrum\n",
    "    spec_energy_acc = np.sum(np.square(np.abs(acc_fft)))/len(acc_fft)\n",
    "    spec_energy_ppg = np.sum(np.square(np.abs(ppg_fft)))/len(ppg_fft)\n",
    "\n",
    "    # Return a list. Take into account that last two elements of this lists are\n",
    "    # also lists. These two elements won't be used to train, but to calculate\n",
    "    # the SNR (the metric for confidence).\n",
    "    f_list = [corr_zp,\n",
    "              ppg_f_maxfft1,\n",
    "              ppg_f_maxfft2,\n",
    "              acc_f_maxfft1,\n",
    "              acc_f_maxfft2,\n",
    "              spec_energy_acc,\n",
    "              spec_energy_ppg,\n",
    "              ppg_freqs,\n",
    "              ppg_fft]\n",
    "\n",
    "    return f_list\n",
    "\n",
    "\n",
    "def GetDataset(data_fls, ref_fls):\n",
    "    \"\"\"\n",
    "    Get entire dataset in memory\n",
    "\n",
    "    Inputs:\n",
    "        data_fls: List of names of the .mat files that contain signal data\n",
    "        ref_fls: List of names of the .mat files that contain reference data\n",
    "\n",
    "    Returns:\n",
    "        features: (np-array) Features of the data per window\n",
    "        labels: (np-array) True label containing heart rate per window in bpm\n",
    "    \"\"\"\n",
    "    # Initialize lists\n",
    "    features, labels = [], []\n",
    "\n",
    "    # Loop through each file\n",
    "    for data_fl, ref_fl in zip(data_fls, ref_fls):\n",
    "        # Load data file\n",
    "        signal = LoadTroikaDataFile(data_fl)\n",
    "\n",
    "        # Load references (labels)\n",
    "        ref = np.array([f[0] for f in scipy.io.loadmat(ref_fl)['BPM0']])\n",
    "\n",
    "        # Loop through each window in the current file\n",
    "        for j, ix in enumerate(range(0, len(signal[0]) - W_LENGTH_N,\n",
    "                                     W_SHIFT_N)):\n",
    "\n",
    "            # Get per window arrays of the signals and band-pass filter them\n",
    "            ppg, accx, accy, accz = BPFilter((F_MIN, F_MAX),\n",
    "                                             *Window(ix, *signal))\n",
    "\n",
    "            # Get per window label\n",
    "            labels.append(ref[j])\n",
    "\n",
    "            # Extract features and record as a list\n",
    "            features.append(Featurize(ppg, accx, accy, accz, FS))\n",
    "\n",
    "    # Convert features and labels into numpy arrays\n",
    "    features, labels = np.array(features, dtype=object), np.array(labels)\n",
    "\n",
    "    # Pack everything into a dataframe\n",
    "    df = pd.DataFrame(features).assign(labels=labels)\n",
    "    c_names = ['corr_zp', 'ppg_f_maxfft1', 'ppg_f_maxfft2', 'acc_f_maxfft1',\n",
    "               'acc_f_maxfft2', 'spec_energy_acc', 'spec_energy_ppg',\n",
    "               'ppg_freqs', 'ppg_fft', 'labels']\n",
    "    df.columns = c_names\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def HoldOut(df, **kwargs):\n",
    "    \"\"\"\n",
    "    Train algorithm splitting dataset into train and test\n",
    "\n",
    "    Inputs:\n",
    "        df: Dataframe with features and labels\n",
    "        kwargs: (dict) Customizable keyword arguments\n",
    "            reg: (str) If 'decision_tree' applies a decision tree algorithm,\n",
    "                otherwise applies random forest. Options:\n",
    "                'decision_tree' | 'random_forest'\n",
    "            t_size: (float) Test size in proportion over 1.\n",
    "            max_depth: (int) Max depth of trees\n",
    "            n_est: (int) Number of estimators in Random Forest regressor.\n",
    "\n",
    "    Returns:\n",
    "        errors: List of errors\n",
    "        confidence: List of confidence values\n",
    "    \"\"\"\n",
    "    # kwargs and defaults\n",
    "    d_tree = True if kwargs['reg'] == 'decision_tree' else False\n",
    "    t_size = kwargs['t_size'] if 't_size' in kwargs else 0.2\n",
    "    max_depth = kwargs['max_depth'] if 'max_depth' in kwargs else 8\n",
    "    n_est = kwargs['n_est'] if 'n_est' in kwargs else 200\n",
    "\n",
    "    # Split data into train and test. Carefull! With time series we usually\n",
    "    # don't shuffle due to the expected auto-correlation in the data. We train\n",
    "    # on past to predict future; but this is not the case, where we just want\n",
    "    # to get heart rate from a time window, no matter what.\n",
    "    train, test = train_test_split(df, test_size=t_size, shuffle=True,\n",
    "                                   random_state=SEED)\n",
    "    X_train = train.iloc[:, :-3]\n",
    "    y_train = train.iloc[:, -1]\n",
    "    X_test = test.iloc[:, :-3]\n",
    "\n",
    "    # Choose and train the model\n",
    "    if d_tree:\n",
    "        # if True, train a Decission Tree Regressor\n",
    "        model = DecisionTreeRegressor(\n",
    "            max_depth=max_depth, random_state=SEED).fit(X_train, y_train)\n",
    "    else:\n",
    "        # If false, train a Random Forest Regressor\n",
    "        model = RandomForestRegressor(n_estimators=n_est, max_depth=max_depth,\n",
    "                                      random_state=SEED).fit(X_train, y_train)\n",
    "\n",
    "    # Get predictions and put them into the test dataframe\n",
    "    test = test.assign(pred=model.predict(X_test))\n",
    "\n",
    "    # Calculate confidence with SNR and turn into list\n",
    "    confidence = test.apply(\n",
    "        lambda x: CalcSNR(x['ppg_freqs'], x['ppg_fft'], x['pred']),\n",
    "        axis=1).to_numpy()\n",
    "\n",
    "    # Calculate errors and turn into list\n",
    "    errors = test.apply(\n",
    "        lambda x: np.abs(x['labels'] - x['pred']), axis=1).to_numpy()\n",
    "\n",
    "    return errors, confidence\n",
    "\n",
    "\n",
    "def CrossValidation(df, **kwargs):\n",
    "    \"\"\"\n",
    "    Train algorithm splitting dataset into k folds and applying CV.\n",
    "\n",
    "    Inputs:\n",
    "        df: Dataframe with features and labels\n",
    "        kwargs: (dict) Customizable keyword arguments\n",
    "            reg: (str) If 'decision_tree' applies a decision tree algorithm,\n",
    "                otherwise applies random forest. Options:\n",
    "                'decision_tree' | 'random_forest'\n",
    "            t_size: (float) Test size in proportion over 1.\n",
    "            max_depth: (int) Max depth of trees\n",
    "            n_est: (int) Number of estimators in Random Forest regressor.\n",
    "\n",
    "    Returns:\n",
    "        errors: List of errors\n",
    "        confidence: List of confidence values\n",
    "    \"\"\"\n",
    "    # kwargs and defaults\n",
    "    d_tree = True if kwargs['reg'] == 'decision_tree' else False\n",
    "    k_folds = kwargs['k_folds'] if 'k_folds' in kwargs else 5\n",
    "    max_depth = kwargs['max_depth'] if 'max_depth' in kwargs else 8\n",
    "    n_est = kwargs['n_est'] if 'n_est' in kwargs else 200\n",
    "\n",
    "    # Prepare lists\n",
    "    errors, confidence = [], []\n",
    "\n",
    "    # Run a train through each CV fold combination. Carefull! With time series\n",
    "    # we usually don't shuffle due to the expected auto-correlation in the\n",
    "    # data. We train on past to predict future; but this is not the case,\n",
    "    # where we just want to get heart rate from a time window, no matter what.\n",
    "    for trn_ix, tst_ix in KFold(n_splits=k_folds, shuffle=True,\n",
    "                                random_state=SEED).split(df):\n",
    "        # Get splits\n",
    "        train, test = df.iloc[trn_ix, :], df.iloc[tst_ix, :]\n",
    "        X_train = train.iloc[:, :-3]\n",
    "        y_train = train.iloc[:, -1]\n",
    "        X_test = test.iloc[:, :-3]\n",
    "\n",
    "        # Choose and train the model\n",
    "        if d_tree:\n",
    "            # if True, train a Decission Tree Regressor\n",
    "            model = DecisionTreeRegressor(\n",
    "                max_depth=max_depth, random_state=SEED).fit(X_train, y_train)\n",
    "        else:\n",
    "            # If false, train a Random Forest Regressor\n",
    "            model = RandomForestRegressor(\n",
    "                n_estimators=n_est, random_state=SEED, max_depth=max_depth\n",
    "                ).fit(X_train, y_train)\n",
    "\n",
    "        # Get predictions and put them into the test dataframe\n",
    "        test = test.assign(pred=model.predict(X_test))\n",
    "\n",
    "        # Calculate confidence with SNR and append to main list\n",
    "        confidence += test.apply(\n",
    "            lambda x: CalcSNR(x['ppg_freqs'], x['ppg_fft'], x['pred']),\n",
    "            axis=1).tolist()\n",
    "\n",
    "        # Calculate errors and append to the main list\n",
    "        errors += test.apply(\n",
    "            lambda x: np.abs(x['labels'] - x['pred']), axis=1).tolist()\n",
    "\n",
    "        # Drop predictions column from the dataframe as preparation\n",
    "        # for next cross validation iteration\n",
    "        test.drop(['pred'], axis=1)\n",
    "\n",
    "    return np.array(errors), np.array(confidence)\n",
    "\n",
    "\n",
    "def Evaluate(**kwargs):\n",
    "    \"\"\"\n",
    "    Top-level function evaluation function.\n",
    "\n",
    "    Runs the pulse rate algorithm on the Troika dataset and returns an\n",
    "    aggregate error metric.\n",
    "\n",
    "    Inputs:\n",
    "        kwargs: (dict) Customizable keyword arguments\n",
    "            reg: 'decision_tree' | 'random_forest'\n",
    "            val: 'hold_out' | 'cross_validation\n",
    "    Returns:\n",
    "        Pulse rate error on the Troika dataset. See AggregateErrorMetric.\n",
    "    \"\"\"\n",
    "    # Keyword arguments and defaults\n",
    "    kwargs['reg'] = kwargs['reg'] if 'reg' in kwargs else 'decision_tree'\n",
    "    kwargs['val'] = kwargs['val'] if 'val' in kwargs else 'hold_out'\n",
    "\n",
    "    # Retrieve dataset\n",
    "    data = GetDataset(*LoadTroikaDataset())\n",
    "\n",
    "    # Run pulse rate extractor algorithm\n",
    "    error, confidence = HoldOut(data, **kwargs) if kwargs['val'] == \\\n",
    "        'hold_out' else CrossValidation(data, **kwargs)\n",
    "\n",
    "    # Compute and return aggregate error metric\n",
    "    return AggregateErrorMetric(error, confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE at 90% availability: 9.1782\n"
     ]
    }
   ],
   "source": [
    "r = Evaluate()\n",
    "print('MAE at 90% availability: {:.4f}'.format(r))\n",
    "\n",
    "# Default values\n",
    "# Simulation with decision tree regressor and hold out\n",
    "# r = Evaluate(reg='decision_tree', val='hold_out', t_size=0.2, max_depth=8)\n",
    "# print('MAE at 90% availability: {:.4f}'.format(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE at 90% availability: 9.8985\n"
     ]
    }
   ],
   "source": [
    "# Simulation with decision tree regressor and cross validation\n",
    "r = Evaluate(reg='decision_tree', val='cross_validation', k_fold=5,\n",
    "             max_depth=8)\n",
    "print('MAE at 90% availability: {:.4f}'.format(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE at 90% availability: 7.9402\n"
     ]
    }
   ],
   "source": [
    "# Simulation with random forest regressor and hold_out\n",
    "r = Evaluate(reg='random_forest', val='hold_out', t_size=0.2, max_depth=8,\n",
    "             n_est=200)\n",
    "print('MAE at 90% availability: {:.4f}'.format(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE at 90% availability: 8.6263\n"
     ]
    }
   ],
   "source": [
    "# Simulation with random forest regressor and cross validation\n",
    "r = Evaluate(reg='random_forest', val='cross_validation', k_fold=5,\n",
    "             max_depth=8, n_est=200)\n",
    "print('MAE at 90% availability: {:.4f}'.format(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Project Write-up\n",
    "\n",
    "Answer the following prompts to demonstrate understanding of the algorithm you wrote for this specific context.\n",
    "\n",
    "> - **Code Description** - Include details so someone unfamiliar with your project will know how to run your code and use your algorithm. \n",
    "> - **Data Description** - Describe the dataset that was used to train and test the algorithm. Include its short-comings and what data would be required to build a more complete dataset.\n",
    "> - **Algorithhm Description** will include the following:\n",
    ">   - how the algorithm works\n",
    ">   - the specific aspects of the physiology that it takes advantage of\n",
    ">   - a description of the algorithm outputs\n",
    ">   - caveats on algorithm outputs \n",
    ">   - common failure modes\n",
    "> - **Algorithm Performance** - Detail how performance was computed (eg. using cross-validation or train-test split) and what metrics were optimized for. Include error metrics that would be relevant to users of your algorithm. Caveat your performance numbers by acknowledging how generalizable they may or may not be on different datasets.\n",
    "\n",
    "Your write-up goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Description\n",
    "\n",
    "1. You can run the algorithm just executing the cell with `Evaluate()`. There's no parameter required.\n",
    "\n",
    "2. The function above uses the Troika[1] dataset to train a Pulse Rate estimator.\n",
    "\n",
    "3. The funtion returns the MAE at 90% availability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data description\n",
    "\n",
    "The dataset can be found here: https://ieeexplore.ieee.org/document/6905737\n",
    "\n",
    "This dataset has ben obtained from 12 subjects with ages from 18 to 35. They were monitored during running. Each subject ran on a treadmill with changing speeds. For datasets with names containing 'TYPE01', the running speeds changed as follows:\n",
    "\n",
    " * rest(30s) -> 8km/h(1min) -> 15km/h(1min) -> 8km/h(1min) -> 15km/h(1min) -> rest(30s)\n",
    "\n",
    "For datasets with names containing 'TYPE02', the running speeds changed as follows:\n",
    "\n",
    " * rest(30s) -> 6km/h(1min) -> 12km/h(1min) -> 6km/h(1min) -> 12km/h(1min) -> rest(30s)\n",
    "\n",
    "During running a two-channel PPG signals, three-axis acceleration signals, and one-channel ECG signals were simultaneously recorded from subjects. For each subject, the PPG signals were recorded from wrist by two pulse oximeters with green LEDs (wavelength: 515nm). Their distance (from center to center) was 2 cm. The acceleration signal was also recorded from wrist by a three-axis accelerometer. Both the pulse oximeter and the accelerometer were embedded in a wristband, which was comfortably worn. The ECG signal was recorded simultaneously from the chest using wet ECG sensors. All signals were sampled at 125 Hz.\n",
    "\n",
    "Each dataset with the similar name 'DATA_01_TYPE01' contains a variable 'sig'. It has 6 rows. The first row is a simultaneous recording of ECG, which is recorded from the chest of each subject. The second row and the third row are two channels of PPG, which are recorded from the wrist of each subject. The last three rows are simultaneous recordings of acceleration data (in x-, y-, and z-axis).\n",
    "\n",
    "For each dataset with the similar name 'DATA_01_TYPE01', the ground-truth of heart rate can be calculated from the simultaneously recorded ECG signal (i.e. the first row of the variable 'sig'). For convenience, the calculated ground-truth heart rate is also provided stored in the datasets with the corresponding name, say 'REF_01_TYPE01'. In each of this kind of datasets, there is a variable 'BPM0',\n",
    "which gives the BPM value in every 8-second time window. Note that two successive time windows overlap by 6 seconds. Thus the first value in 'BPM0' gives the calcualted heart rate ground-truth in\n",
    "the first 8 seconds, while the second value in 'BPM0' gives the calculated heart rate ground-truth from the 3rd second to the 10th second.\n",
    "\n",
    "For more complete dataset more data would be required. Probably the ideal situation would be having a lot more of subjects in the study with different ages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm description\n",
    "\n",
    "See image below to have a general overview of the function calls \n",
    "\n",
    "![Schema](schema2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this schema image, the algorithm works as follows:\n",
    "\n",
    "When running `Evaluate()` it gets the dataset filenames (`LoadTroikaDataset`) and get all the data in memory through `GetDataset`. `RunPulseRateAlgorithm` has been eliminated and substituted by two possible functions depending on parameters (`HoldOut` or `CrossValidation`). By default `HoldOut` is used. There it selects the split mode and launch the training. Finally, `AggregateMetric` calculates the output metric as a result.\n",
    "\n",
    "`GetDataset` loads each file (`LoadTroikaDataFile`), filter the data with a passband filter (`BPFilter`) and loops over all possible windows in the file through `Window`. In each window data are extracted with `Featurize` and `Fourier`.\n",
    "\n",
    "In the training part, once selected the default `HoldOut`, the regressor is trained. During training, whether it's through `HoldOut` or `CrossValidation`, the prediction of heart rate is excuted and the errors and confidences are computed (the confidence is computed through `CalcSNR` function, because SNR is the metric used for confidence. \n",
    "\n",
    "The algorithm uses PPG signals to estimate heart rate. In addition, it takes advantage of having the accelerometer signal to avoid considering as heart rate frequencies related to the motion.\n",
    "\n",
    "The output of the algorithm is the error (MAE) at 90% availability. In addition, the estimated heart rate can be obtained from the function `HoldOut` or `CrossValidation` (depending on the selection).\n",
    "\n",
    "The 90% confidence is calculated directly getting the 90 percentile SNR values. The error is the Mean Absolute Error.\n",
    "\n",
    "There are other factors that can add noise to the heart rate signal, such as degree of melanin, arm movement or position, and finger movement. These factors aren't taken into account in this algorithm.\n",
    "\n",
    "The algorithm performance is calculated through the MAE at 90% availability. The result (the MAE) in the test is better if Hold Out technique is used. Random Forest seems to lightly outperform Decision Tree with default parameters. Even so, the test dataset was simpler than the training dataset, so that the results could be better than those achieved in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Next Steps\n",
    "You will now go to **Test Your Algorithm** (back in the Project Classroom) to apply a unit test to confirm that your algorithm met the success criteria. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "7c7c13d78fab5266de4164a014492f9fb29a2ac31e29792fe523b528a7450d7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
